{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing all the required libraries\nimport os\nimport pickle\nimport string\nimport tensorflow\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom keras.layers.merge import add\nfrom keras.models import Model,load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.layers import Input,Dense,LSTM,Embedding,Dropout\nfrom keras.preprocessing.image import img_to_array,load_img\nfrom nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:39.852199Z","iopub.execute_input":"2022-02-22T17:14:39.852604Z","iopub.status.idle":"2022-02-22T17:14:47.077961Z","shell.execute_reply.started":"2022-02-22T17:14:39.852567Z","shell.execute_reply":"2022-02-22T17:14:47.076968Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#finding the path and length the of image data \nimages_path = '../input/flickr8k-sau/Flickr_Data/Images/'\nimages = glob(images_path+'*.jpg')\nlen(images)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:47.080490Z","iopub.execute_input":"2022-02-22T17:14:47.080847Z","iopub.status.idle":"2022-02-22T17:14:47.391942Z","shell.execute_reply.started":"2022-02-22T17:14:47.080812Z","shell.execute_reply":"2022-02-22T17:14:47.390914Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Read the first 10 images from the entire dataset which consists of 8091 image files\nimages[:10]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:47.393463Z","iopub.execute_input":"2022-02-22T17:14:47.393786Z","iopub.status.idle":"2022-02-22T17:14:47.399992Z","shell.execute_reply.started":"2022-02-22T17:14:47.393756Z","shell.execute_reply":"2022-02-22T17:14:47.399021Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#let us import the cv2 module and plot the images in range of 5\nimport cv2\nfor i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:47.401597Z","iopub.execute_input":"2022-02-22T17:14:47.401899Z","iopub.status.idle":"2022-02-22T17:14:48.768643Z","shell.execute_reply.started":"2022-02-22T17:14:47.401858Z","shell.execute_reply":"2022-02-22T17:14:48.767843Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Now here we use resnet50 to predict the input image\nfrom keras.applications import ResNet50\n\ninception_model = ResNet50(weights = None,include_top=True,input_shape = (220,220,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:48.771625Z","iopub.execute_input":"2022-02-22T17:14:48.772080Z","iopub.status.idle":"2022-02-22T17:14:50.787454Z","shell.execute_reply.started":"2022-02-22T17:14:48.772030Z","shell.execute_reply":"2022-02-22T17:14:50.786624Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#let us find the layers and the functionality of our dataset\nfrom keras.models import Model\nlast = inception_model.layers[-2].output\nmodele = Model(inputs = inception_model.input,outputs = last)\nmodele.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:50.790020Z","iopub.execute_input":"2022-02-22T17:14:50.790508Z","iopub.status.idle":"2022-02-22T17:14:50.865801Z","shell.execute_reply.started":"2022-02-22T17:14:50.790457Z","shell.execute_reply":"2022-02-22T17:14:50.864874Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Here we split the model and reshape it\nimages_features = {}\ncount = 0\nfor i in images:\n    img = cv2.imread(i)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (220,220))\n    \n    img = img.reshape(1,220,220,3)\n    pred = modele.predict(img).reshape(2048,)\n        \n    img_name = i.split('/')[-1]\n    \n    images_features[img_name] = pred\n    \n    count += 1\n    \n    if count > 1499:\n        break\n        \n    elif count % 50 == 0:\n        print(count)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:14:50.867196Z","iopub.execute_input":"2022-02-22T17:14:50.867477Z","iopub.status.idle":"2022-02-22T17:18:38.959881Z","shell.execute_reply.started":"2022-02-22T17:14:50.867449Z","shell.execute_reply":"2022-02-22T17:18:38.958815Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"******Now from here we have to deal with captions and text preprocessing","metadata":{}},{"cell_type":"code","source":"#let us read the caption path\ncaption_path = '../input/flickr8k-sau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:38.961033Z","iopub.execute_input":"2022-02-22T17:18:38.961336Z","iopub.status.idle":"2022-02-22T17:18:38.965735Z","shell.execute_reply.started":"2022-02-22T17:18:38.961306Z","shell.execute_reply":"2022-02-22T17:18:38.964658Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#open the file in binary format for reading and then decode it\ncaptions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:38.967381Z","iopub.execute_input":"2022-02-22T17:18:38.967810Z","iopub.status.idle":"2022-02-22T17:18:39.043679Z","shell.execute_reply.started":"2022-02-22T17:18:38.967767Z","shell.execute_reply":"2022-02-22T17:18:39.042625Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#find the lenght of the captions\nlen(captions)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:39.045569Z","iopub.execute_input":"2022-02-22T17:18:39.045909Z","iopub.status.idle":"2022-02-22T17:18:39.052498Z","shell.execute_reply.started":"2022-02-22T17:18:39.045878Z","shell.execute_reply":"2022-02-22T17:18:39.051317Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"captions_dict = {}\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2] \n        caption = i.split('\\t')[1]\n        if img_name in images_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption]\n                \n            else:\n                captions_dict[img_name].append(caption)\n            \n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:39.054417Z","iopub.execute_input":"2022-02-22T17:18:39.054851Z","iopub.status.idle":"2022-02-22T17:18:39.116673Z","shell.execute_reply.started":"2022-02-22T17:18:39.054807Z","shell.execute_reply":"2022-02-22T17:18:39.115621Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#finding the length of captions_dict\nlen(captions_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:39.118215Z","iopub.execute_input":"2022-02-22T17:18:39.118714Z","iopub.status.idle":"2022-02-22T17:18:39.125190Z","shell.execute_reply.started":"2022-02-22T17:18:39.118671Z","shell.execute_reply":"2022-02-22T17:18:39.124253Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Now let us visualize the images with its captions**","metadata":{}},{"cell_type":"code","source":"#for this we use opencv module(cv2)\nfor i in range(5):\n    plt.figure()\n    img_name = images[i]\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('/')[-1]])\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:39.126505Z","iopub.execute_input":"2022-02-22T17:18:39.126796Z","iopub.status.idle":"2022-02-22T17:18:40.247897Z","shell.execute_reply.started":"2022-02-22T17:18:39.126768Z","shell.execute_reply":"2022-02-22T17:18:40.247060Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.248997Z","iopub.execute_input":"2022-02-22T17:18:40.249399Z","iopub.status.idle":"2022-02-22T17:18:40.254218Z","shell.execute_reply.started":"2022-02-22T17:18:40.249360Z","shell.execute_reply":"2022-02-22T17:18:40.253250Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"for k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.255749Z","iopub.execute_input":"2022-02-22T17:18:40.256219Z","iopub.status.idle":"2022-02-22T17:18:40.274759Z","shell.execute_reply.started":"2022-02-22T17:18:40.256172Z","shell.execute_reply":"2022-02-22T17:18:40.273623Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"count_words = {}\nfor k,vv in captions_dict.items():\n    for v in vv:\n        for word in v.split():\n            if word not in count_words:\n\n                count_words[word] = 0\n\n            else:\n                count_words[word] += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.276171Z","iopub.execute_input":"2022-02-22T17:18:40.276498Z","iopub.status.idle":"2022-02-22T17:18:40.331135Z","shell.execute_reply.started":"2022-02-22T17:18:40.276465Z","shell.execute_reply":"2022-02-22T17:18:40.330071Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#creating the new dictionary\nTHRESH = -1\ncount = 1\nnew_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        new_dict[k] = count\n        count += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.332395Z","iopub.execute_input":"2022-02-22T17:18:40.333001Z","iopub.status.idle":"2022-02-22T17:18:40.340539Z","shell.execute_reply.started":"2022-02-22T17:18:40.332963Z","shell.execute_reply":"2022-02-22T17:18:40.339629Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(new_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.342317Z","iopub.execute_input":"2022-02-22T17:18:40.342769Z","iopub.status.idle":"2022-02-22T17:18:40.357690Z","shell.execute_reply.started":"2022-02-22T17:18:40.342724Z","shell.execute_reply":"2022-02-22T17:18:40.356598Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"new_dict['<OUT>'] = len(new_dict) ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.359269Z","iopub.execute_input":"2022-02-22T17:18:40.359895Z","iopub.status.idle":"2022-02-22T17:18:40.366168Z","shell.execute_reply.started":"2022-02-22T17:18:40.359848Z","shell.execute_reply":"2022-02-22T17:18:40.365135Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"captions_backup = captions_dict.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.368120Z","iopub.execute_input":"2022-02-22T17:18:40.368641Z","iopub.status.idle":"2022-02-22T17:18:40.376886Z","shell.execute_reply.started":"2022-02-22T17:18:40.368597Z","shell.execute_reply":"2022-02-22T17:18:40.376010Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"captions_dict = captions_backup.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.378403Z","iopub.execute_input":"2022-02-22T17:18:40.378823Z","iopub.status.idle":"2022-02-22T17:18:40.387120Z","shell.execute_reply.started":"2022-02-22T17:18:40.378782Z","shell.execute_reply":"2022-02-22T17:18:40.386271Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"for k, vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():  \n            if word not in new_dict:\n                encoded.append(new_dict['<OUT>'])\n            else:\n                encoded.append(new_dict[word])\n\n\n        captions_dict[k][vv.index(v)] = encoded","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.388295Z","iopub.execute_input":"2022-02-22T17:18:40.388595Z","iopub.status.idle":"2022-02-22T17:18:40.446409Z","shell.execute_reply.started":"2022-02-22T17:18:40.388565Z","shell.execute_reply":"2022-02-22T17:18:40.445618Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"len(captions_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.448641Z","iopub.execute_input":"2022-02-22T17:18:40.449077Z","iopub.status.idle":"2022-02-22T17:18:40.454886Z","shell.execute_reply.started":"2022-02-22T17:18:40.449034Z","shell.execute_reply":"2022-02-22T17:18:40.454062Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"captions_dict","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:40.455980Z","iopub.execute_input":"2022-02-22T17:18:40.456423Z","iopub.status.idle":"2022-02-22T17:18:42.174948Z","shell.execute_reply.started":"2022-02-22T17:18:40.456382Z","shell.execute_reply":"2022-02-22T17:18:42.173885Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Now we create the generator function**","metadata":{}},{"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:42.176875Z","iopub.execute_input":"2022-02-22T17:18:42.177301Z","iopub.status.idle":"2022-02-22T17:18:42.182912Z","shell.execute_reply.started":"2022-02-22T17:18:42.177256Z","shell.execute_reply":"2022-02-22T17:18:42.181897Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 0\nfor k, vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:42.184809Z","iopub.execute_input":"2022-02-22T17:18:42.185204Z","iopub.status.idle":"2022-02-22T17:18:42.199652Z","shell.execute_reply.started":"2022-02-22T17:18:42.185162Z","shell.execute_reply":"2022-02-22T17:18:42.198418Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"MAX_LEN","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:42.201254Z","iopub.execute_input":"2022-02-22T17:18:42.202060Z","iopub.status.idle":"2022-02-22T17:18:42.210703Z","shell.execute_reply.started":"2022-02-22T17:18:42.202014Z","shell.execute_reply":"2022-02-22T17:18:42.209589Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"Batch_size = 5000\nVOCAB_SIZE = len(new_dict)\n\ndef generator(photo, caption):\n    n_samples = 0\n    \n    X = []\n    y_in = []\n    y_out = []\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range(1, len(v)):\n                X.append(photo[k])\n\n                in_seq= [v[:i]]\n                out_seq = v[i]\n\n                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n            \n    return X, y_in, y_out","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:42.212604Z","iopub.execute_input":"2022-02-22T17:18:42.213264Z","iopub.status.idle":"2022-02-22T17:18:42.223326Z","shell.execute_reply.started":"2022-02-22T17:18:42.213219Z","shell.execute_reply":"2022-02-22T17:18:42.222576Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"X, y_in, y_out = generator(images_features, captions_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:42.224591Z","iopub.execute_input":"2022-02-22T17:18:42.224884Z","iopub.status.idle":"2022-02-22T17:18:47.841905Z","shell.execute_reply.started":"2022-02-22T17:18:42.224855Z","shell.execute_reply":"2022-02-22T17:18:47.840955Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#we find the length of all the parameters\nlen(X), len(y_in), len(y_out)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:47.843106Z","iopub.execute_input":"2022-02-22T17:18:47.843395Z","iopub.status.idle":"2022-02-22T17:18:47.849726Z","shell.execute_reply.started":"2022-02-22T17:18:47.843367Z","shell.execute_reply":"2022-02-22T17:18:47.848739Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#Now we assign the data type to generate the desired output\nX = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:47.851031Z","iopub.execute_input":"2022-02-22T17:18:47.851344Z","iopub.status.idle":"2022-02-22T17:18:49.953025Z","shell.execute_reply.started":"2022-02-22T17:18:47.851310Z","shell.execute_reply":"2022-02-22T17:18:49.951978Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Now we find the shape of all three parameters\nX.shape, y_in.shape, y_out.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:49.955292Z","iopub.execute_input":"2022-02-22T17:18:49.955684Z","iopub.status.idle":"2022-02-22T17:18:49.963110Z","shell.execute_reply.started":"2022-02-22T17:18:49.955647Z","shell.execute_reply":"2022-02-22T17:18:49.961719Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**Now we build the model for predictions**","metadata":{}},{"cell_type":"code","source":"#Here we import all the required libraries from keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:49.964525Z","iopub.execute_input":"2022-02-22T17:18:49.964803Z","iopub.status.idle":"2022-02-22T17:18:49.975600Z","shell.execute_reply.started":"2022-02-22T17:18:49.964775Z","shell.execute_reply":"2022-02-22T17:18:49.974531Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#Here we concatenate image model and language model for prediction purpose and finding the sequential model\nembedding_size = 120\nmax_len = MAX_LEN\nvocab_size = len(new_dict)\n\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"../input/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:49.976826Z","iopub.execute_input":"2022-02-22T17:18:49.977166Z","iopub.status.idle":"2022-02-22T17:18:50.988373Z","shell.execute_reply.started":"2022-02-22T17:18:49.977129Z","shell.execute_reply":"2022-02-22T17:18:50.987317Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#fit the model into testing test\nmodel.fit([X, y_in], y_out, batch_size=500, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T17:18:50.993960Z","iopub.execute_input":"2022-02-22T17:18:50.994261Z","iopub.status.idle":"2022-02-22T18:36:01.546011Z","shell.execute_reply.started":"2022-02-22T17:18:50.994231Z","shell.execute_reply":"2022-02-22T18:36:01.544741Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"inv_dict = {v:k for k, v in new_dict.items()}","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:36:01.550551Z","iopub.execute_input":"2022-02-22T18:36:01.550942Z","iopub.status.idle":"2022-02-22T18:36:01.561280Z","shell.execute_reply.started":"2022-02-22T18:36:01.550893Z","shell.execute_reply":"2022-02-22T18:36:01.560571Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:36:01.562461Z","iopub.execute_input":"2022-02-22T18:36:01.562973Z","iopub.status.idle":"2022-02-22T18:36:01.683525Z","shell.execute_reply.started":"2022-02-22T18:36:01.562940Z","shell.execute_reply":"2022-02-22T18:36:01.682477Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model.save_weights('mine_model_weights.h5')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:36:01.684827Z","iopub.execute_input":"2022-02-22T18:36:01.685245Z","iopub.status.idle":"2022-02-22T18:36:01.729010Z","shell.execute_reply.started":"2022-02-22T18:36:01.685204Z","shell.execute_reply":"2022-02-22T18:36:01.727902Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"np.save('vocab.npy', new_dict)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:36:01.730486Z","iopub.execute_input":"2022-02-22T18:36:01.730827Z","iopub.status.idle":"2022-02-22T18:36:01.737450Z","shell.execute_reply.started":"2022-02-22T18:36:01.730795Z","shell.execute_reply":"2022-02-22T18:36:01.736615Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Test the images\ndef getImage(x):\n    \n    test_img_path = images[x]\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (224,224))\n\n    test_img = np.reshape(test_img, (1,224,224,3))\n    \n    return test_img","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:36:01.739128Z","iopub.execute_input":"2022-02-22T18:36:01.739783Z","iopub.status.idle":"2022-02-22T18:36:01.747309Z","shell.execute_reply.started":"2022-02-22T18:36:01.739739Z","shell.execute_reply":"2022-02-22T18:36:01.746322Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"**Prediction of the caption for the input image**","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    \n    no = np.random.randint(1000,2000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(new_dict[i])\n\n        encoded = [encoded]\n\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T18:39:44.121061Z","iopub.execute_input":"2022-02-22T18:39:44.121417Z","iopub.status.idle":"2022-02-22T18:39:50.822099Z","shell.execute_reply.started":"2022-02-22T18:39:44.121385Z","shell.execute_reply":"2022-02-22T18:39:50.821257Z"},"trusted":true},"execution_count":48,"outputs":[]}]}